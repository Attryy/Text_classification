{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This project aims to classify wine on the basis of wine tasting reviews with some text analysis and modeling. The data set is publicly available on [Kaggle](https://www.kaggle.com/zynicide/wine-reviews). There are in total around 130K records in the data set. The summary of the columns are presented below.\n",
    "\n",
    "* **country** : The country that the wine is from\n",
    "* **description**: description of the taster\n",
    "* **designation**: The vineyard within the winery where the grapes that made the wine are from\n",
    "* **points**: The number of points WineEnthusiast rated the wine on a scale of 1-100 (though they say they only post reviews for wines that score >= 80)\n",
    "* **price**: The cost for a bottle of the wine\n",
    "* **province**: The province or state that the wine is from\n",
    "* **region_1**: The wine growing area in a province or state (eg: Napa)\n",
    "* **region_2**: Sometimes there are more specific regions specified within a wine growing area (ie Rutherford inside the Napa Valley), but this value can sometimes be blank\n",
    "* **taster_name**: name of the taster\n",
    "* **taster_twitter_handle**: twitter handle for the taster\n",
    "* **title**: The title of the wine review, which often contains the vintage if you're interested in extracting that feature\n",
    "* **variety**: The type of grapes used to make the wine (eg: Pinot Noir)\n",
    "* **winery**: The winery that made the wine\n",
    "\n",
    "\n",
    "We are going to use the **description** column as the input and predict the varieties of the wine from the labels in **variety** column.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn import svm \n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from collections import defaultdict\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "import nltk\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load the dataset from the corresponding directory and obtain an overview of it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/home/yiwei/yiwei_data/winemag-data-130k-v2.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 129971 entries, 0 to 129970\n",
      "Data columns (total 13 columns):\n",
      "country                  129908 non-null object\n",
      "description              129971 non-null object\n",
      "designation              92506 non-null object\n",
      "points                   129971 non-null int64\n",
      "price                    120975 non-null float64\n",
      "province                 129908 non-null object\n",
      "region_1                 108724 non-null object\n",
      "region_2                 50511 non-null object\n",
      "taster_name              103727 non-null object\n",
      "taster_twitter_handle    98758 non-null object\n",
      "title                    129971 non-null object\n",
      "variety                  129970 non-null object\n",
      "winery                   129971 non-null object\n",
      "dtypes: float64(1), int64(1), object(11)\n",
      "memory usage: 13.9+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove the one record with *NULL* variety label and keep only the **description** and **variety** for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ['description', 'variety']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[pd.notnull(df['variety'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are $707$ unique wine varieties in total. We only take the top 25 varieties as categories for prediction and remove those records whose variety is not among the top 25. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "707"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['variety'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_25 = df.groupby('variety').count().sort_values('description', ascending = False)[0:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>variety</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Pinot Noir</th>\n",
       "      <td>13272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chardonnay</th>\n",
       "      <td>11753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cabernet Sauvignon</th>\n",
       "      <td>9472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Red Blend</th>\n",
       "      <td>8946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bordeaux-style Red Blend</th>\n",
       "      <td>6915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Riesling</th>\n",
       "      <td>5189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sauvignon Blanc</th>\n",
       "      <td>4967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Syrah</th>\n",
       "      <td>4142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rosé</th>\n",
       "      <td>3564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Merlot</th>\n",
       "      <td>3102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nebbiolo</th>\n",
       "      <td>2804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zinfandel</th>\n",
       "      <td>2714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sangiovese</th>\n",
       "      <td>2707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Malbec</th>\n",
       "      <td>2652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Portuguese Red</th>\n",
       "      <td>2466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>White Blend</th>\n",
       "      <td>2360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sparkling Blend</th>\n",
       "      <td>2153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tempranillo</th>\n",
       "      <td>1810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rhône-style Red Blend</th>\n",
       "      <td>1471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pinot Gris</th>\n",
       "      <td>1455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Champagne Blend</th>\n",
       "      <td>1396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cabernet Franc</th>\n",
       "      <td>1353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Grüner Veltliner</th>\n",
       "      <td>1345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Portuguese White</th>\n",
       "      <td>1159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bordeaux-style White Blend</th>\n",
       "      <td>1066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            description\n",
       "variety                                \n",
       "Pinot Noir                        13272\n",
       "Chardonnay                        11753\n",
       "Cabernet Sauvignon                 9472\n",
       "Red Blend                          8946\n",
       "Bordeaux-style Red Blend           6915\n",
       "Riesling                           5189\n",
       "Sauvignon Blanc                    4967\n",
       "Syrah                              4142\n",
       "Rosé                               3564\n",
       "Merlot                             3102\n",
       "Nebbiolo                           2804\n",
       "Zinfandel                          2714\n",
       "Sangiovese                         2707\n",
       "Malbec                             2652\n",
       "Portuguese Red                     2466\n",
       "White Blend                        2360\n",
       "Sparkling Blend                    2153\n",
       "Tempranillo                        1810\n",
       "Rhône-style Red Blend              1471\n",
       "Pinot Gris                         1455\n",
       "Champagne Blend                    1396\n",
       "Cabernet Franc                     1353\n",
       "Grüner Veltliner                   1345\n",
       "Portuguese White                   1159\n",
       "Bordeaux-style White Blend         1066"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = {variety: num for num, variety in enumerate(top_25.index.tolist())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#{value: key for key, value in label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Bordeaux-style Red Blend': 4,\n",
       " 'Bordeaux-style White Blend': 24,\n",
       " 'Cabernet Franc': 21,\n",
       " 'Cabernet Sauvignon': 2,\n",
       " 'Champagne Blend': 20,\n",
       " 'Chardonnay': 1,\n",
       " 'Grüner Veltliner': 22,\n",
       " 'Malbec': 13,\n",
       " 'Merlot': 9,\n",
       " 'Nebbiolo': 10,\n",
       " 'Pinot Gris': 19,\n",
       " 'Pinot Noir': 0,\n",
       " 'Portuguese Red': 14,\n",
       " 'Portuguese White': 23,\n",
       " 'Red Blend': 3,\n",
       " 'Rhône-style Red Blend': 18,\n",
       " 'Riesling': 5,\n",
       " 'Rosé': 8,\n",
       " 'Sangiovese': 12,\n",
       " 'Sauvignon Blanc': 6,\n",
       " 'Sparkling Blend': 16,\n",
       " 'Syrah': 7,\n",
       " 'Tempranillo': 17,\n",
       " 'White Blend': 15,\n",
       " 'Zinfandel': 11}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top = df[df['variety'].isin(label.keys())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_list = df_top['description'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "variety_list = [label[i] for i in df_top['variety'].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(description_list, variety_list, test_size=0.3, random_state = 1216)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70163"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30070"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are going to process the raw text data in description and tranform the text into feature vectors. The following ideas would be explored. \n",
    "\n",
    "* TF-IDF vectors on word level as features: we use the regular expression [\\w\\\\\\%\\']+ to tokenize the text with accents and specify the stopword set as the stopwords from NLTK library. Futhermore, we ignore the tokens that appear in less than $3$ descriptions when building the vocabulary by setting *min_df* = $3$.\n",
    "* TF-IDF vectors on N-gram level as features: we use the same *token_pattern* as TF-IDF on word level and set the ngram_range to $2$ to $3$.  \n",
    "* word embedding vectors as features: we use the pre-trained Glove word embeddings with IDF weightings to build feature vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF on word level\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=3,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words={\"doesn't\", 'himself', 'being', 'what', 'to', \"shan't\", 'any', 'up', 'if', 'and', \"didn't\", \"aren't\", \"needn't\", 'are', 'hadn', 're', 'that', \"she's\", \"hasn't\", 'we', 'doesn', 'before', 'until', 'again', 'more', \"shouldn't\", 'd', 'herself', 'here', 'most', 'those', 'such', 'not', 'am', 'o...', 'down', 'wasn', 'hasn', \"you're\", 'you', 't', \"don't\", 'hers', 'some', 'they', 'she', 'so', 'of'},\n",
       "        strip_accents='unicode', sublinear_tf=False,\n",
       "        token_pattern=\"[\\\\w\\\\%']+\", tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vect = TfidfVectorizer(analyzer = 'word',token_pattern='[\\w\\\\%\\']+', strip_accents = 'unicode', stop_words=set(nltk.corpus.stopwords.words('english')), min_df = 3)\n",
    "tfidf_vect.fit(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/yiwei/yiwei_data/vectorizerDesc.pk', 'wb') as file:\n",
    "     pickle.dump(tfidf_vect, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12618"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xtest_tfidf =  tfidf_vect.transform(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF on N-gram level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=3,\n",
       "        ngram_range=(2, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words={\"doesn't\", 'himself', 'being', 'what', 'to', \"shan't\", 'any', 'up', 'if', 'and', \"didn't\", \"aren't\", \"needn't\", 'are', 'hadn', 're', 'that', \"she's\", \"hasn't\", 'we', 'doesn', 'before', 'until', 'again', 'more', \"shouldn't\", 'd', 'herself', 'here', 'most', 'those', 'such', 'not', 'am', 'o...', 'down', 'wasn', 'hasn', \"you're\", 'you', 't', \"don't\", 'hers', 'some', 'they', 'she', 'so', 'of'},\n",
       "        strip_accents='unicode', sublinear_tf=False,\n",
       "        token_pattern=\"[\\\\w\\\\%']+\", tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern='[\\w\\\\%\\']+',strip_accents = 'unicode', ngram_range=(2,3), stop_words=set(nltk.corpus.stopwords.words('english')), min_df = 3)\n",
    "tfidf_vect_ngram.fit(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160671"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf_vect_ngram.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "xtest_ngram =  tfidf_vect_ngram.transform(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the pre-trained Glove word embeddings to tranform each word in the text into a $100$-dimensional vectors. First of all, we load the embeddings as a dictionary with $400000$ key-value pairs. The easiest way to build features with word embeddings is to average the word vectors for all words in the text. Here we first tokenize the text using tfidf_vect (also remove stopwords) and then use the inverse document frequency(IDF) as the weightings for the word vectors to obtain feature vectors. \n",
    "\n",
    "**Note**: to deal with the words that have never been seen, we set the default weighting for unseen words to be maximum of all the IDF's as it has to be less frequently seen than any of the known words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/yiwei/yiwei_data/glove.6B.100d.txt\", \"rb\") as lines:\n",
    "    wordVec = {line.split()[0].decode('utf-8'): np.array(list(map(float, line.split()[1:])))\n",
    "           for line in lines}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the tokenizer from tfidf_vect\n",
    "text_tokenizer = tfidf_vect.build_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_idf = max(tfidf_vect.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = defaultdict(lambda: max_idf, [(token, tfidf_vect.idf_[i]) for token, i  in tfidf_vect.vocabulary_.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_tokens  = [text_tokenizer(doc) for doc in train_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_embedding =  np.array([\n",
    "        np.mean([wordVec[token]*weight[token] for token in text if token in wordVec] \n",
    "                 or [np.zeros(100)], axis=0)\n",
    "        for text in xtrain_tokens\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest_tokens  = [text_tokenizer(doc) for doc in test_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest_embedding =  np.array([\n",
    "        np.mean([wordVec[token]*weight[token] for token in text if token in wordVec] \n",
    "                 or [np.zeros(100)], axis=0)\n",
    "        for text in xtest_tokens\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We explore three classical classifiers for this problem:\n",
    "* multinomial Naive Bayes classifier;\n",
    "* support vector machine (SVM);\n",
    "* neural networks\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid writing repeated code, we put together a function for fitting different models with different feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(clf, feature_train, label_train, feature_test, label_test, name):\n",
    "\n",
    "    clf.fit(feature_train, label_train)\n",
    "    \n",
    "    # save the model to disk\n",
    "    filename = '/home/yiwei/yiwei_data/'+name+'.sav'\n",
    "    pickle.dump(clf, open(filename, 'wb'))\n",
    "    \n",
    "    y_predict = clf.predict(feature_test)\n",
    "    \n",
    "    \n",
    "    return metrics.accuracy_score(y_predict, label_test )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Multinomial Naive Bayes Accuracy with TF-IDF on word level: %0.4f' % train_model(MultinomialNB(), xtrain_tfidf, train_y, xtest_tfidf, test_y, name = 'MultiNB'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes Accuracy with TF-IDF on N-gram level: 0.4738\n"
     ]
    }
   ],
   "source": [
    "print('Multinomial Naive Bayes Accuracy with TF-IDF on N-gram level: %0.4f' % train_model(MultinomialNB(), xtrain_ngram, train_y, xtest_ngram, test_y, name = 'MultiNB_ngram'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because Multinomial Naive Bayes is intended for non-negative input, we use Gaussian Naive Bayes classifier to predict the wine variety based on the word embedding vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes Accuracy with word embedding: 0.1473\n"
     ]
    }
   ],
   "source": [
    "print('Multinomial Naive Bayes Accuracy with word embedding: %0.4f' % train_model(GaussianNB(), xtrain_embedding, train_y, xtest_embedding, test_y, name = 'MultiNB_embedding'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy with TF-IDF on word level: 0.7132\n"
     ]
    }
   ],
   "source": [
    "print('SVM Accuracy with TF-IDF on word level: %0.4f' % train_model(svm.SVC(kernel = 'linear'), xtrain_tfidf, train_y, xtest_tfidf, test_y, name = 'svcModel'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SVM Accuracy with TF-IDF on N-gram level: %0.4f' % train_model(svm.SVC(kernel = 'linear'), xtrain_ngram, train_y, xtest_ngram, test_y, name = 'svcModel_ngram'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM accuracy with TF-IDF on N-gram level : 0.6262"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SVM Accuracy with word embedding: %0.4f' % train_model(svm.SVC(kernel = 'linear'), xtrain_embedding, train_y, xtest_embedding, test_y, name = 'svcEmbedding'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare the input data for the neural network, we first examine the length of the tokens in the training data and pad the sequence with Keras function *pad_sequences* with *maxlen* = $200$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12618"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(doc) for doc in xtrain_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizerKeras= Tokenizer(num_words= 20000, filters='!\"#$&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True, split=' ', char_level=False, oov_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizerKeras.fit_on_texts(xtrain_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_seq = sequence.pad_sequences(tokenizerKeras.texts_to_sequences(xtrain_tokens), maxlen=200,  dtype='int32', padding='pre', truncating='pre', value=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest_seq = sequence.pad_sequences(tokenizerKeras.texts_to_sequences(xtest_tokens), maxlen=200,  dtype='int32', padding='pre', truncating='pre', value=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70163, 200)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70163, 25)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70163, 200)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_encoded = to_categorical(train_y)\n",
    "test_y_encoded = to_categorical(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(input_dim = 20000, output_dim = 64, input_length = 200))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, input_shape=(64,),activation='relu'))\n",
    "model.add(Dense(25, input_shape= (64,) ,activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 200, 64)           1280000   \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 12800)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 64)                819264    \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 25)                1625      \n",
      "=================================================================\n",
      "Total params: 2,100,889\n",
      "Trainable params: 2,100,889\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping callback\n",
    "# early stopping tells the model to stop thw training when it doesn't see any performance improvement for a user-defined \n",
    "# time-frame (the number of epoch with the PATIENCE parameter)\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "PATIENCE = 10\n",
    "early_stopping = EarlyStopping(monitor='loss', min_delta=0, patience=PATIENCE, verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 63146 samples, validate on 7017 samples\n",
      "Epoch 1/50\n",
      "63146/63146 [==============================] - 3s 46us/step - loss: 1.8926 - acc: 0.4281 - val_loss: 1.3296 - val_acc: 0.5834\n",
      "Epoch 2/50\n",
      "63146/63146 [==============================] - 3s 43us/step - loss: 1.0501 - acc: 0.6749 - val_loss: 1.1369 - val_acc: 0.6605\n",
      "Epoch 3/50\n",
      "63146/63146 [==============================] - 3s 43us/step - loss: 0.7435 - acc: 0.7729 - val_loss: 1.1153 - val_acc: 0.6692\n",
      "Epoch 4/50\n",
      "63146/63146 [==============================] - 3s 43us/step - loss: 0.5164 - acc: 0.8481 - val_loss: 1.1688 - val_acc: 0.6711\n",
      "Epoch 5/50\n",
      "63146/63146 [==============================] - 3s 43us/step - loss: 0.3278 - acc: 0.9102 - val_loss: 1.2966 - val_acc: 0.6638\n",
      "Epoch 6/50\n",
      "63146/63146 [==============================] - 3s 43us/step - loss: 0.1872 - acc: 0.9541 - val_loss: 1.4720 - val_acc: 0.6558\n",
      "Epoch 7/50\n",
      "63146/63146 [==============================] - 3s 43us/step - loss: 0.0954 - acc: 0.9811 - val_loss: 1.6677 - val_acc: 0.6484\n",
      "Epoch 8/50\n",
      "63146/63146 [==============================] - 3s 43us/step - loss: 0.0432 - acc: 0.9943 - val_loss: 1.8661 - val_acc: 0.6441\n",
      "Epoch 9/50\n",
      "63146/63146 [==============================] - 3s 43us/step - loss: 0.0180 - acc: 0.9989 - val_loss: 2.0307 - val_acc: 0.6440\n",
      "Epoch 10/50\n",
      "63146/63146 [==============================] - 3s 43us/step - loss: 0.0076 - acc: 0.9999 - val_loss: 2.1861 - val_acc: 0.6434\n",
      "Epoch 11/50\n",
      "63146/63146 [==============================] - 3s 43us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 2.3280 - val_acc: 0.6396\n",
      "Epoch 12/50\n",
      "63146/63146 [==============================] - 3s 43us/step - loss: 0.0022 - acc: 1.0000 - val_loss: 2.4360 - val_acc: 0.6424\n",
      "Epoch 13/50\n",
      "63146/63146 [==============================] - 3s 43us/step - loss: 0.0015 - acc: 1.0000 - val_loss: 2.5469 - val_acc: 0.6419\n",
      "Epoch 14/50\n",
      "63146/63146 [==============================] - 3s 43us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 2.6440 - val_acc: 0.6396\n",
      "Epoch 15/50\n",
      "63146/63146 [==============================] - 3s 43us/step - loss: 9.9705e-04 - acc: 1.0000 - val_loss: 2.7433 - val_acc: 0.6399\n",
      "Epoch 16/50\n",
      "63146/63146 [==============================] - 3s 43us/step - loss: 8.9366e-04 - acc: 1.0000 - val_loss: 2.8389 - val_acc: 0.6392\n",
      "Epoch 17/50\n",
      "63146/63146 [==============================] - 3s 43us/step - loss: 8.3715e-04 - acc: 1.0000 - val_loss: 2.9322 - val_acc: 0.6382\n",
      "Epoch 18/50\n",
      "63146/63146 [==============================] - 3s 43us/step - loss: 8.0608e-04 - acc: 1.0000 - val_loss: 3.0179 - val_acc: 0.6387\n",
      "Epoch 19/50\n",
      "63146/63146 [==============================] - 3s 43us/step - loss: 7.8859e-04 - acc: 1.0000 - val_loss: 3.1057 - val_acc: 0.6372\n",
      "Epoch 20/50\n",
      "63146/63146 [==============================] - 3s 43us/step - loss: 7.7878e-04 - acc: 1.0000 - val_loss: 3.1867 - val_acc: 0.6365\n",
      "Epoch 21/50\n",
      "63146/63146 [==============================] - 3s 43us/step - loss: 7.7328e-04 - acc: 1.0000 - val_loss: 3.2610 - val_acc: 0.6359\n",
      "Epoch 22/50\n",
      "63146/63146 [==============================] - 3s 43us/step - loss: 7.7020e-04 - acc: 1.0000 - val_loss: 3.3331 - val_acc: 0.6355\n",
      "Epoch 23/50\n",
      "63146/63146 [==============================] - 3s 43us/step - loss: 7.6842e-04 - acc: 1.0000 - val_loss: 3.3976 - val_acc: 0.6347\n",
      "Epoch 24/50\n",
      "63146/63146 [==============================] - 3s 43us/step - loss: 7.6738e-04 - acc: 1.0000 - val_loss: 3.4603 - val_acc: 0.6365\n",
      "Epoch 25/50\n",
      "63146/63146 [==============================] - 3s 43us/step - loss: 7.6678e-04 - acc: 1.0000 - val_loss: 3.5185 - val_acc: 0.6350\n",
      "Epoch 26/50\n",
      "63146/63146 [==============================] - 3s 43us/step - loss: 7.6642e-04 - acc: 1.0000 - val_loss: 3.5694 - val_acc: 0.6343\n",
      "Epoch 27/50\n",
      "63146/63146 [==============================] - 3s 43us/step - loss: 7.6621e-04 - acc: 1.0000 - val_loss: 3.6237 - val_acc: 0.6332\n",
      "Epoch 28/50\n",
      "63146/63146 [==============================] - 3s 43us/step - loss: 7.6608e-04 - acc: 1.0000 - val_loss: 3.6660 - val_acc: 0.6349\n",
      "Epoch 29/50\n",
      "63146/63146 [==============================] - 3s 43us/step - loss: 7.6600e-04 - acc: 1.0000 - val_loss: 3.7065 - val_acc: 0.6343\n",
      "Epoch 30/50\n",
      "63146/63146 [==============================] - 3s 43us/step - loss: 7.6595e-04 - acc: 1.0000 - val_loss: 3.7454 - val_acc: 0.6339\n",
      "Epoch 31/50\n",
      "63146/63146 [==============================] - 3s 43us/step - loss: 7.6592e-04 - acc: 1.0000 - val_loss: 3.7764 - val_acc: 0.6333\n",
      "Epoch 32/50\n",
      "63146/63146 [==============================] - 3s 43us/step - loss: 7.6590e-04 - acc: 1.0000 - val_loss: 3.8024 - val_acc: 0.6336\n",
      "Epoch 33/50\n",
      "63146/63146 [==============================] - 3s 43us/step - loss: 7.6589e-04 - acc: 1.0000 - val_loss: 3.8279 - val_acc: 0.6336\n",
      "Epoch 34/50\n",
      "63146/63146 [==============================] - 3s 43us/step - loss: 7.6588e-04 - acc: 1.0000 - val_loss: 3.8472 - val_acc: 0.6325\n",
      "Epoch 35/50\n",
      "63146/63146 [==============================] - 3s 43us/step - loss: 7.6588e-04 - acc: 1.0000 - val_loss: 3.8632 - val_acc: 0.6340\n",
      "Epoch 36/50\n",
      "63146/63146 [==============================] - 3s 43us/step - loss: 7.6588e-04 - acc: 1.0000 - val_loss: 3.8770 - val_acc: 0.6325\n",
      "Epoch 37/50\n",
      "63146/63146 [==============================] - 3s 43us/step - loss: 7.6588e-04 - acc: 1.0000 - val_loss: 3.8838 - val_acc: 0.6340\n",
      "Epoch 38/50\n",
      "63146/63146 [==============================] - 3s 43us/step - loss: 7.6587e-04 - acc: 1.0000 - val_loss: 3.8985 - val_acc: 0.6335\n",
      "Epoch 39/50\n",
      "63146/63146 [==============================] - 3s 43us/step - loss: 7.6587e-04 - acc: 1.0000 - val_loss: 3.9028 - val_acc: 0.6332\n",
      "Epoch 40/50\n",
      "63146/63146 [==============================] - 3s 43us/step - loss: 7.6587e-04 - acc: 1.0000 - val_loss: 3.9129 - val_acc: 0.6332\n",
      "Epoch 41/50\n",
      "63146/63146 [==============================] - 3s 43us/step - loss: 7.6587e-04 - acc: 1.0000 - val_loss: 3.9113 - val_acc: 0.6337\n",
      "Epoch 42/50\n",
      "63146/63146 [==============================] - 3s 43us/step - loss: 7.6587e-04 - acc: 1.0000 - val_loss: 3.9141 - val_acc: 0.6332\n",
      "Epoch 43/50\n",
      "63146/63146 [==============================] - 3s 43us/step - loss: 7.6587e-04 - acc: 1.0000 - val_loss: 3.9179 - val_acc: 0.6332\n",
      "Epoch 44/50\n",
      "63146/63146 [==============================] - 3s 43us/step - loss: 7.6587e-04 - acc: 1.0000 - val_loss: 3.9242 - val_acc: 0.6330\n",
      "Epoch 45/50\n",
      "63146/63146 [==============================] - 3s 43us/step - loss: 7.6587e-04 - acc: 1.0000 - val_loss: 3.9263 - val_acc: 0.6332\n",
      "Epoch 46/50\n",
      "63146/63146 [==============================] - 3s 43us/step - loss: 7.6587e-04 - acc: 1.0000 - val_loss: 3.9284 - val_acc: 0.6330\n",
      "Epoch 47/50\n",
      "63146/63146 [==============================] - 3s 43us/step - loss: 7.6587e-04 - acc: 1.0000 - val_loss: 3.9272 - val_acc: 0.6337\n",
      "Epoch 48/50\n",
      "63146/63146 [==============================] - 3s 43us/step - loss: 7.6587e-04 - acc: 1.0000 - val_loss: 3.9311 - val_acc: 0.6326\n",
      "Epoch 49/50\n",
      "63146/63146 [==============================] - 3s 43us/step - loss: 7.6587e-04 - acc: 1.0000 - val_loss: 3.9292 - val_acc: 0.6330\n",
      "Epoch 50/50\n",
      "63146/63146 [==============================] - 3s 43us/step - loss: 7.6587e-04 - acc: 1.0000 - val_loss: 3.9370 - val_acc: 0.6330\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6e59e465f8>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain_seq, train_y_encoded, validation_split = 0.10, epochs=50, batch_size= 64, callbacks = [early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose the appropriate epoch based on the validation loss and validation accuracy. What can also be fine-tuned is the batch size with cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "70163/70163 [==============================] - 6s 79us/step - loss: 1.7040 - acc: 0.4818\n",
      "Epoch 2/3\n",
      "70163/70163 [==============================] - 5s 76us/step - loss: 0.9391 - acc: 0.7081\n",
      "Epoch 3/3\n",
      "70163/70163 [==============================] - 5s 76us/step - loss: 0.6293 - acc: 0.8058\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6e107769e8>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain_seq, train_y_encoded, epochs= 3, batch_size= 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30070/30070 [==============================] - 1s 36us/step\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accu = model.evaluate(xtest_seq, test_y_encoded, batch_size= 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accu: 0.6744928500324855\n",
      "test_loss: 1.1032199547224992\n"
     ]
    }
   ],
   "source": [
    "print('test_accu:', test_accu)\n",
    "print('test_loss:', test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final model trained on all data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect.fit(description_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tfidf =  tfidf_vect.transform(description_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel = 'linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit( x_tfidf, variety_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/home/yiwei/yiwei_data/SVMmodel.sav'\n",
    "    pickle.dump(clf, open(filename, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
